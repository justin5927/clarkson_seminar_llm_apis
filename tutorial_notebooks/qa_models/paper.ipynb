{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the document or webpage in this case\n",
    "\n",
    "Note - Unstrutured.io might be under api soon so you could load webpage or load document in other ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=[\"https://arxiv.org/pdf/2207.05566.pdf\"])\n",
    "url_data = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(url_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute embeddings and create vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings() # Defaults to ada\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create QA Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-3.5-turbo\"), chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of this paper are Isha Hameed, Samuel Sharpe, Daniel Barcklow, Justin Au-Yeung, Sahil Verma, Jocelyn Huang, Brian Barr, and C. Bayan Bruss.\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"who are the authors on this paper\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper is about conducting ablation studies for explainable artificial intelligence (XAI) on tabular data. The authors propose a framework for applying ablation, which involves perturbing features of a trained model based on their importance and assessing the model's performance. They also discuss the importance of selecting appropriate baselines in XAI methods and provide guidelines for baseline selection. The paper aims to provide a more rigorous approach for conducting ablation studies on tabular data and raises questions for future work in the field of XAI.\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"what is the paper about\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation refers to a process in which certain features or components of a system are intentionally removed or disabled in order to study their individual contributions or effects on the overall system. In the context of machine learning and artificial intelligence, an ablation study involves systematically removing or perturbing input features to understand their impact on the performance or behavior of a trained model. This process helps in evaluating the validity and importance of different features in the model's decision-making process. Ablation studies are often used in the field of explainable artificial intelligence (XAI) to assess the interpretability and robustness of models.\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"what is ablation\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of the given information, a guardrail refers to a visual representation or measurement used to assess the quality and effectiveness of explanations in an ablation study for Explainable Artificial Intelligence (XAI). It serves as a reference point or boundary for evaluating the importance or relevance of features in the study. There are three types of guardrails mentioned: horizontal guardrail, vertical guardrail, and random guardrail. These guardrails help in understanding the performance and interpretability of the ablation study results.\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"what is a guardrail\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main points of the paper are as follows:\n",
      "\n",
      "1. The need for explainability of black box models arises from the requirement for understanding how these models make predictions.\n",
      "\n",
      "2. A comprehensive understanding of which methods and hyperparameters are best for a particular use case is lacking due to the lack of comprehensive ground truth sources for local attributions.\n",
      "\n",
      "3. Ablation studies can assess the effectiveness of local and global attributions by measuring the sensitivity of model capability under perturbed inputs.\n",
      "\n",
      "4. Baseline selection in XAI methods is crucial, as different baselines can significantly impact the generated feature attributions.\n",
      "\n",
      "5. Baselines that deviate from the original data generating distribution can produce invalid explanations, leading to out-of-distribution (OOD) data.\n",
      "\n",
      "6. The current experiments in the field lack thoroughness, particularly in the selection of appropriate baseline methods for tabular data.\n",
      "\n",
      "7. The paper proposes the use of ablation studies to assess the validity of local or global explanations by examining the relative model performance change as input features are perturbed in order of importance.\n",
      "\n",
      "8. Differentiable models are trained for each ablation study, and gradient-based explanation methods like Deep SHAP, Integrated Gradients, and Kernel SHAP are used to generate local explanations.\n",
      "\n",
      "9. Global explanations are obtained by averaging the absolute values of the local explanations.\n",
      "\n",
      "10. Traditional XAI methods that naively permute or vary features without considering interactions between them are being criticized, and alternative approaches are suggested, such as sampling features conditionally or dropping features and retraining.\n",
      "\n",
      "11. Ablation studies have been used to assess XAI methods in image processing models, and the impact of different baselines on explanations has been investigated.\n",
      "\n",
      "12. The paper focuses on the application of ablation studies to tabular data and highlights the limitations of existing experiments in this domain.\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"what are the main points of the paper\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study used two-layer neural network models for each ablation study.\n"
     ]
    }
   ],
   "source": [
    "response = qa.run(\"what type of models were used for this study\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note entirely correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
